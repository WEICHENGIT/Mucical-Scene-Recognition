{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "usage: __main__.py [-h] [-development] [-challenge] [-v]\n",
      "__main__.py: error: unrecognized arguments: -f C:\\Users\\Win7-Wei\\AppData\\Roaming\\jupyter\\runtime\\kernel-3a486b7b-2236-4571-b659-8c73a213d0d9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# DCASE 2016::Acoustic Scene Classification / Baseline System\n",
    "\n",
    "from src.ui import *\n",
    "from src.general import *\n",
    "from src.files import *\n",
    "\n",
    "from src.features import *\n",
    "from src.dataset import *\n",
    "from src.evaluation import *\n",
    "\n",
    "import numpy\n",
    "import csv\n",
    "import argparse\n",
    "import textwrap\n",
    "import copy\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "__version_info__ = ('1', '0', '0')\n",
    "__version__ = '.'.join(__version_info__)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    numpy.random.seed(123456)  # let's make randomization predictable\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prefix_chars='-+',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        description=textwrap.dedent('''\\\n",
    "            DCASE 2016\n",
    "            Task 1: Acoustic Scene Classification\n",
    "            Baseline system\n",
    "            ---------------------------------------------\n",
    "                Tampere University of Technology / Audio Research Group\n",
    "                Author:  Toni Heittola ( toni.heittola@tut.fi )\n",
    "\n",
    "            System description\n",
    "                This is an baseline implementation for D-CASE 2016 challenge acoustic scene classification task.\n",
    "                Features: MFCC (static+delta+acceleration)\n",
    "                Classifier: GMM\n",
    "\n",
    "        '''))\n",
    "\n",
    "    # Setup argument handling\n",
    "    parser.add_argument(\"-development\", help=\"Use the system in the development mode\", action='store_true',\n",
    "                        default=False, dest='development')\n",
    "    parser.add_argument(\"-challenge\", help=\"Use the system in the challenge mode\", action='store_true',\n",
    "                        default=False, dest='challenge')\n",
    "\n",
    "    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load parameters from config file\n",
    "    parameter_file = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n",
    "                                  os.path.splitext(os.path.basename(__file__))[0]+'.yaml')\n",
    "    params = load_parameters(parameter_file)\n",
    "    params = process_parameters(params)\n",
    "    make_folders(params)\n",
    "\n",
    "    title(\"DCASE 2016::Acoustic Scene Classification / Baseline System\")\n",
    "\n",
    "    # Check if mode is defined\n",
    "    if not (args.development or args.challenge):\n",
    "        args.development = True\n",
    "        args.challenge = False\n",
    "\n",
    "    dataset_evaluation_mode = 'folds'\n",
    "    if args.development and not args.challenge:\n",
    "        print (\"Running system in development mode\")\n",
    "        dataset_evaluation_mode = 'folds'\n",
    "    elif not args.development and args.challenge:\n",
    "        print (\"Running system in challenge mode\")\n",
    "        dataset_evaluation_mode = 'full'\n",
    "\n",
    "    # Get dataset container class\n",
    "    dataset = eval(params['general']['development_dataset'])(data_path=params['path']['data'])\n",
    "\n",
    "    # Fetch data over internet and setup the data\n",
    "    # ==================================================\n",
    "    if params['flow']['initialize']:\n",
    "        dataset.fetch()\n",
    "\n",
    "    # Extract features for all audio files in the dataset\n",
    "    # ==================================================\n",
    "    if params['flow']['extract_features']:\n",
    "        section_header('Feature extraction')\n",
    "\n",
    "        # Collect files in train sets and test sets\n",
    "        files = []\n",
    "        for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "            for item_id, item in enumerate(dataset.train(fold)):\n",
    "                if item['file'] not in files:\n",
    "                    files.append(item['file'])\n",
    "            for item_id, item in enumerate(dataset.test(fold)):\n",
    "                if item['file'] not in files:\n",
    "                    files.append(item['file'])\n",
    "        files = sorted(files)\n",
    "\n",
    "        # Go through files and make sure all features are extracted\n",
    "        do_feature_extraction(files=files,\n",
    "                              dataset=dataset,\n",
    "                              feature_path=params['path']['features'],\n",
    "                              params=params['features'],\n",
    "                              overwrite=params['general']['overwrite'])\n",
    "\n",
    "        foot()\n",
    "\n",
    "    # Prepare feature normalizers\n",
    "    # ==================================================\n",
    "    if params['flow']['feature_normalizer']:\n",
    "        section_header('Feature normalizer')\n",
    "\n",
    "        do_feature_normalization(dataset=dataset,\n",
    "                                 feature_normalizer_path=params['path']['feature_normalizers'],\n",
    "                                 feature_path=params['path']['features'],\n",
    "                                 dataset_evaluation_mode=dataset_evaluation_mode,\n",
    "                                 overwrite=params['general']['overwrite'])\n",
    "\n",
    "        foot()\n",
    "\n",
    "    # System training\n",
    "    # ==================================================\n",
    "    if params['flow']['train_system']:\n",
    "        section_header('System training')\n",
    "\n",
    "        do_system_training(dataset=dataset,                           \n",
    "                           model_path=params['path']['models'],\n",
    "                           feature_normalizer_path=params['path']['feature_normalizers'],\n",
    "                           feature_path=params['path']['features'],\n",
    "                           feature_params=params['features'],\n",
    "                           classifier_params=params['classifier']['parameters'],\n",
    "                           classifier_method=params['classifier']['method'],\n",
    "                           dataset_evaluation_mode=dataset_evaluation_mode,\n",
    "                           clean_audio_errors=params['classifier']['audio_error_handling']['clean_data'],\n",
    "                           overwrite=params['general']['overwrite']\n",
    "                           )\n",
    "\n",
    "        foot()\n",
    "\n",
    "    # System evaluation in development mode\n",
    "    if args.development and not args.challenge:\n",
    "\n",
    "        # System testing\n",
    "        # ==================================================\n",
    "        if params['flow']['test_system']:\n",
    "            section_header('System testing')\n",
    "\n",
    "            do_system_testing(dataset=dataset,                              \n",
    "                              feature_path=params['path']['features'],\n",
    "                              result_path=params['path']['results'],\n",
    "                              model_path=params['path']['models'],\n",
    "                              feature_params=params['features'],\n",
    "                              dataset_evaluation_mode=dataset_evaluation_mode,\n",
    "                              classifier_method=params['classifier']['method'],\n",
    "                              clean_audio_errors=params['recognizer']['audio_error_handling']['clean_data'],\n",
    "                              overwrite=params['general']['overwrite']\n",
    "                              )\n",
    "            \n",
    "            foot()\n",
    "\n",
    "        # System evaluation\n",
    "        # ==================================================\n",
    "        if params['flow']['evaluate_system']:\n",
    "            section_header('System evaluation')\n",
    "\n",
    "            do_system_evaluation(dataset=dataset,\n",
    "                                 dataset_evaluation_mode=dataset_evaluation_mode,\n",
    "                                 result_path=params['path']['results'])\n",
    "\n",
    "            foot()\n",
    "\n",
    "    # System evaluation with challenge data\n",
    "    elif not args.development and args.challenge:\n",
    "        # Fetch data over internet and setup the data\n",
    "        challenge_dataset = eval(params['general']['challenge_dataset'])(data_path=params['path']['data'])\n",
    "        if params['general']['challenge_submission_mode']:\n",
    "            result_path = params['path']['challenge_results']\n",
    "        else:\n",
    "            result_path = params['path']['results']\n",
    "\n",
    "        if params['flow']['initialize']:\n",
    "            challenge_dataset.fetch()\n",
    "\n",
    "        if not params['general']['challenge_submission_mode']:\n",
    "            section_header('Feature extraction for challenge data')\n",
    "\n",
    "            # Extract feature if not running in challenge submission mode.\n",
    "            # Collect test files\n",
    "            files = []\n",
    "            for fold in challenge_dataset.folds(mode=dataset_evaluation_mode):\n",
    "                for item_id, item in enumerate(dataset.test(fold)):\n",
    "                    if item['file'] not in files:\n",
    "                        files.append(item['file'])\n",
    "            files = sorted(files)\n",
    "\n",
    "            # Go through files and make sure all features are extracted\n",
    "            do_feature_extraction(files=files,\n",
    "                                  dataset=challenge_dataset,\n",
    "                                  feature_path=params['path']['features'],\n",
    "                                  params=params['features'],\n",
    "                                  overwrite=params['general']['overwrite'])\n",
    "            foot()\n",
    "\n",
    "        # System testing\n",
    "        if params['flow']['test_system']:\n",
    "            section_header('System testing with challenge data')\n",
    "\n",
    "            do_system_testing(dataset=challenge_dataset,\n",
    "                              feature_path=params['path']['features'],\n",
    "                              result_path=result_path,\n",
    "                              model_path=params['path']['models'],\n",
    "                              feature_params=params['features'],\n",
    "                              dataset_evaluation_mode=dataset_evaluation_mode,\n",
    "                              classifier_method=params['classifier']['method'],\n",
    "                              clean_audio_errors=params['recognizer']['audio_error_handling']['clean_data'],\n",
    "                              overwrite=params['general']['overwrite'] or params['general']['challenge_submission_mode']\n",
    "                              )\n",
    "            foot()\n",
    "\n",
    "            if params['general']['challenge_submission_mode']:\n",
    "                print (\" \")\n",
    "                print (\"Your results for the challenge data are stored at [\"+params['path']['challenge_results']+\"]\")\n",
    "                print (\" \")\n",
    "\n",
    "        # System evaluation if not in challenge submission mode\n",
    "        if params['flow']['evaluate_system'] and not params['general']['challenge_submission_mode']:\n",
    "            section_header('System evaluation with challenge data')\n",
    "            do_system_evaluation(dataset=challenge_dataset,\n",
    "                                 dataset_evaluation_mode=dataset_evaluation_mode,\n",
    "                                 result_path=result_path)\n",
    "\n",
    "            foot()\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def process_parameters(params):\n",
    "    \"\"\"Parameter post-processing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        parameters in dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : dict\n",
    "        processed parameters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert feature extraction window and hop sizes seconds to samples\n",
    "    params['features']['mfcc']['win_length'] = int(params['features']['win_length_seconds'] * params['features']['fs'])\n",
    "    params['features']['mfcc']['hop_length'] = int(params['features']['hop_length_seconds'] * params['features']['fs'])\n",
    "\n",
    "    # Copy parameters for current classifier method\n",
    "    params['classifier']['parameters'] = params['classifier_parameters'][params['classifier']['method']]\n",
    "\n",
    "    # Hash\n",
    "    params['features']['hash'] = get_parameter_hash(params['features'])\n",
    "\n",
    "    # Let's keep hashes backwards compatible after added parameters.\n",
    "    # Only if error handling is used, they are included in the hash.\n",
    "    classifier_params = copy.copy(params['classifier'])\n",
    "    if not classifier_params['audio_error_handling']['clean_data']:\n",
    "        del classifier_params['audio_error_handling']\n",
    "    params['classifier']['hash'] = get_parameter_hash(classifier_params)\n",
    "\n",
    "    params['recognizer']['hash'] = get_parameter_hash(params['recognizer'])\n",
    "\n",
    "    # Paths\n",
    "    params['path']['data'] = os.path.join(os.path.dirname(os.path.realpath(__file__)), params['path']['data'])\n",
    "    params['path']['base'] = os.path.join(os.path.dirname(os.path.realpath(__file__)), params['path']['base'])\n",
    "\n",
    "    # Features\n",
    "    params['path']['features_'] = params['path']['features']\n",
    "    params['path']['features'] = os.path.join(params['path']['base'],\n",
    "                                              params['path']['features'],\n",
    "                                              params['features']['hash'])\n",
    "\n",
    "    # Feature normalizers\n",
    "    params['path']['feature_normalizers_'] = params['path']['feature_normalizers']\n",
    "    params['path']['feature_normalizers'] = os.path.join(params['path']['base'],\n",
    "                                                         params['path']['feature_normalizers'],\n",
    "                                                         params['features']['hash'])\n",
    "\n",
    "    # Models\n",
    "    params['path']['models_'] = params['path']['models']\n",
    "    params['path']['models'] = os.path.join(params['path']['base'],\n",
    "                                            params['path']['models'],\n",
    "                                            params['features']['hash'],\n",
    "                                            params['classifier']['hash'])\n",
    "    # Results\n",
    "    params['path']['results_'] = params['path']['results']\n",
    "    params['path']['results'] = os.path.join(params['path']['base'],\n",
    "                                             params['path']['results'],\n",
    "                                             params['features']['hash'],\n",
    "                                             params['classifier']['hash'],\n",
    "                                             params['recognizer']['hash'])\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def make_folders(params, parameter_filename='parameters.yaml'):\n",
    "    \"\"\"Create all needed folders, and saves parameters in yaml-file for easier manual browsing of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        parameters in dict\n",
    "\n",
    "    parameter_filename : str\n",
    "        filename to save parameters used to generate the folder name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that target path exists, create if not\n",
    "    check_path(params['path']['features'])\n",
    "    check_path(params['path']['feature_normalizers'])\n",
    "    check_path(params['path']['models'])\n",
    "    check_path(params['path']['results'])\n",
    "\n",
    "    # Save parameters into folders to help manual browsing of files.\n",
    "\n",
    "    # Features\n",
    "    feature_parameter_filename = os.path.join(params['path']['features'], parameter_filename)\n",
    "    if not os.path.isfile(feature_parameter_filename):\n",
    "        save_parameters(feature_parameter_filename, params['features'])\n",
    "\n",
    "    # Feature normalizers\n",
    "    feature_normalizer_parameter_filename = os.path.join(params['path']['feature_normalizers'], parameter_filename)\n",
    "    if not os.path.isfile(feature_normalizer_parameter_filename):\n",
    "        save_parameters(feature_normalizer_parameter_filename, params['features'])\n",
    "\n",
    "    # Models\n",
    "    model_features_parameter_filename = os.path.join(params['path']['base'],\n",
    "                                                     params['path']['models_'],\n",
    "                                                     params['features']['hash'],\n",
    "                                                     parameter_filename)\n",
    "    if not os.path.isfile(model_features_parameter_filename):\n",
    "        save_parameters(model_features_parameter_filename, params['features'])\n",
    "\n",
    "    model_models_parameter_filename = os.path.join(params['path']['base'],\n",
    "                                                   params['path']['models_'],\n",
    "                                                   params['features']['hash'],\n",
    "                                                   params['classifier']['hash'],\n",
    "                                                   parameter_filename)\n",
    "    if not os.path.isfile(model_models_parameter_filename):\n",
    "        save_parameters(model_models_parameter_filename, params['classifier'])\n",
    "\n",
    "    # Results\n",
    "    # Save parameters into folders to help manual browsing of files.\n",
    "    result_features_parameter_filename = os.path.join(params['path']['base'],\n",
    "                                                      params['path']['results_'],\n",
    "                                                      params['features']['hash'],\n",
    "                                                      parameter_filename)\n",
    "    if not os.path.isfile(result_features_parameter_filename):\n",
    "        save_parameters(result_features_parameter_filename, params['features'])\n",
    "\n",
    "    result_models_parameter_filename = os.path.join(params['path']['base'],\n",
    "                                                    params['path']['results_'],\n",
    "                                                    params['features']['hash'],\n",
    "                                                    params['classifier']['hash'],\n",
    "                                                    parameter_filename)\n",
    "    if not os.path.isfile(result_models_parameter_filename):\n",
    "        save_parameters(result_models_parameter_filename, params['classifier'])\n",
    "\n",
    "    result_models_parameter_filename = os.path.join(params['path']['base'],\n",
    "                                                    params['path']['results_'],\n",
    "                                                    params['features']['hash'],\n",
    "                                                    params['classifier']['hash'],\n",
    "                                                    params['recognizer']['hash'],\n",
    "                                                    parameter_filename)\n",
    "    if not os.path.isfile(result_models_parameter_filename):\n",
    "        save_parameters(result_models_parameter_filename, params['recognizer'])\n",
    "\n",
    "def get_feature_filename(audio_file, path, extension='cpickle'):\n",
    "    \"\"\"Get feature filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_file : str\n",
    "        audio file name from which the features are extracted\n",
    "\n",
    "    path :  str\n",
    "        feature path\n",
    "\n",
    "    extension : str\n",
    "        file extension\n",
    "        (Default value='cpickle')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_filename : str\n",
    "        full feature filename\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    audio_filename = os.path.split(audio_file)[1]\n",
    "    return os.path.join(path, os.path.splitext(audio_filename)[0] + '.' + extension)\n",
    "\n",
    "\n",
    "def get_feature_normalizer_filename(fold, path, extension='cpickle'):\n",
    "    \"\"\"Get normalizer filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fold : int >= 0\n",
    "        evaluation fold number\n",
    "\n",
    "    path :  str\n",
    "        normalizer path\n",
    "\n",
    "    extension : str\n",
    "        file extension\n",
    "        (Default value='cpickle')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    normalizer_filename : str\n",
    "        full normalizer filename\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return os.path.join(path, 'scale_fold' + str(fold) + '.' + extension)\n",
    "\n",
    "\n",
    "def get_model_filename(fold, path, extension='cpickle'):\n",
    "    \"\"\"Get model filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fold : int >= 0\n",
    "        evaluation fold number\n",
    "\n",
    "    path :  str\n",
    "        model path\n",
    "\n",
    "    extension : str\n",
    "        file extension\n",
    "        (Default value='cpickle')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_filename : str\n",
    "        full model filename\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return os.path.join(path, 'model_fold' + str(fold) + '.' + extension)\n",
    "\n",
    "\n",
    "def get_result_filename(fold, path, extension='txt'):\n",
    "    \"\"\"Get result filename\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fold : int >= 0\n",
    "        evaluation fold number\n",
    "\n",
    "    path :  str\n",
    "        result path\n",
    "\n",
    "    extension : str\n",
    "        file extension\n",
    "        (Default value='cpickle')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_filename : str\n",
    "        full result filename\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if fold == 0:\n",
    "        return os.path.join(path, 'results.' + extension)\n",
    "    else:\n",
    "        return os.path.join(path, 'results_fold' + str(fold) + '.' + extension)\n",
    "\n",
    "\n",
    "def do_feature_extraction(files, dataset, feature_path, params, overwrite=False):\n",
    "    \"\"\"Feature extraction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list\n",
    "        file list\n",
    "\n",
    "    dataset : class\n",
    "        dataset class\n",
    "\n",
    "    feature_path : str\n",
    "        path where the features are saved\n",
    "\n",
    "    params : dict\n",
    "        parameter dict\n",
    "\n",
    "    overwrite : bool\n",
    "        overwrite existing feature files\n",
    "        (Default value=False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    IOError\n",
    "        Audio file not found.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that target path exists, create if not\n",
    "    check_path(feature_path)\n",
    "\n",
    "    for file_id, audio_filename in enumerate(files):\n",
    "        # Get feature filename\n",
    "        current_feature_file = get_feature_filename(audio_file=os.path.split(audio_filename)[1], path=feature_path)\n",
    "\n",
    "        progress(title_text='Extracting',\n",
    "                 percentage=(float(file_id) / len(files)),\n",
    "                 note=os.path.split(audio_filename)[1])\n",
    "\n",
    "        if not os.path.isfile(current_feature_file) or overwrite:\n",
    "            # Load audio data\n",
    "            if os.path.isfile(dataset.relative_to_absolute_path(audio_filename)):\n",
    "                y, fs = load_audio(filename=dataset.relative_to_absolute_path(audio_filename), mono=True, fs=params['fs'])\n",
    "            else:\n",
    "                raise IOError(\"Audio file not found [%s]\" % audio_filename)\n",
    "\n",
    "            # Extract features\n",
    "            feature_data = feature_extraction(y=y,\n",
    "                                              fs=fs,\n",
    "                                              include_mfcc0=params['include_mfcc0'],\n",
    "                                              include_delta=params['include_delta'],\n",
    "                                              include_acceleration=params['include_acceleration'],\n",
    "                                              mfcc_params=params['mfcc'],\n",
    "                                              delta_params=params['mfcc_delta'],\n",
    "                                              acceleration_params=params['mfcc_acceleration'])\n",
    "            # Save\n",
    "            save_data(current_feature_file, feature_data)\n",
    "\n",
    "\n",
    "def do_feature_normalization(dataset, feature_normalizer_path, feature_path, dataset_evaluation_mode='folds', overwrite=False):\n",
    "    \"\"\"Feature normalization\n",
    "\n",
    "    Calculated normalization factors for each evaluation fold based on the training material available.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : class\n",
    "        dataset class\n",
    "\n",
    "    feature_normalizer_path : str\n",
    "        path where the feature normalizers are saved.\n",
    "\n",
    "    feature_path : str\n",
    "        path where the features are saved.\n",
    "\n",
    "    dataset_evaluation_mode : str ['folds', 'full']\n",
    "        evaluation mode, 'full' all material available is considered to belong to one fold.\n",
    "        (Default value='folds')\n",
    "\n",
    "    overwrite : bool\n",
    "        overwrite existing normalizers\n",
    "        (Default value=False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    IOError\n",
    "        Feature file not found.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that target path exists, create if not\n",
    "    check_path(feature_normalizer_path)\n",
    "\n",
    "    for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "        current_normalizer_file = get_feature_normalizer_filename(fold=fold, path=feature_normalizer_path)\n",
    "\n",
    "        if not os.path.isfile(current_normalizer_file) or overwrite:\n",
    "            # Initialize statistics\n",
    "            file_count = len(dataset.train(fold))\n",
    "            normalizer = FeatureNormalizer()\n",
    "\n",
    "            for item_id, item in enumerate(dataset.train(fold)):\n",
    "                progress(title_text='Collecting data',\n",
    "                         fold=fold,\n",
    "                         percentage=(float(item_id) / file_count),\n",
    "                         note=os.path.split(item['file'])[1])\n",
    "                # Load features\n",
    "                if os.path.isfile(get_feature_filename(audio_file=item['file'], path=feature_path)):\n",
    "                    feature_data = load_data(get_feature_filename(audio_file=item['file'], path=feature_path))['stat']\n",
    "                else:\n",
    "                    raise IOError(\"Feature file not found [%s]\" % (item['file']))\n",
    "\n",
    "                # Accumulate statistics\n",
    "                normalizer.accumulate(feature_data)\n",
    "            \n",
    "            # Calculate normalization factors\n",
    "            normalizer.finalize()\n",
    "\n",
    "            # Save\n",
    "            save_data(current_normalizer_file, normalizer)\n",
    "\n",
    "\n",
    "def do_system_training(dataset, model_path, feature_normalizer_path, feature_path, feature_params, classifier_params,\n",
    "                       dataset_evaluation_mode='folds', classifier_method='gmm', clean_audio_errors=False, overwrite=False):\n",
    "    \"\"\"System training\n",
    "\n",
    "    model container format:\n",
    "\n",
    "    {\n",
    "        'normalizer': normalizer class\n",
    "        'models' :\n",
    "            {\n",
    "                'office' : mixture.GMM class\n",
    "                'home' : mixture.GMM class\n",
    "                ...\n",
    "            }\n",
    "    }\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : class\n",
    "        dataset class\n",
    "\n",
    "    model_path : str\n",
    "        path where the models are saved.\n",
    "\n",
    "    feature_normalizer_path : str\n",
    "        path where the feature normalizers are saved.\n",
    "\n",
    "    feature_path : str\n",
    "        path where the features are saved.\n",
    "\n",
    "    feature_params : dict\n",
    "        parameter dict\n",
    "\n",
    "    classifier_params : dict\n",
    "        parameter dict\n",
    "\n",
    "    dataset_evaluation_mode : str ['folds', 'full']\n",
    "        evaluation mode, 'full' all material available is considered to belong to one fold.\n",
    "        (Default value='folds')\n",
    "\n",
    "    classifier_method : str ['gmm']\n",
    "        classifier method, currently only GMM supported\n",
    "        (Default value='gmm')\n",
    "\n",
    "    clean_audio_errors : bool\n",
    "        Remove audio errors from the training data\n",
    "        (Default value=False)\n",
    "\n",
    "    overwrite : bool\n",
    "        overwrite existing models\n",
    "        (Default value=False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    ValueError\n",
    "        classifier_method is unknown.\n",
    "\n",
    "    IOError\n",
    "        Feature normalizer not found.\n",
    "        Feature file not found.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if classifier_method != 'gmm':\n",
    "        raise ValueError(\"Unknown classifier method [\"+classifier_method+\"]\")\n",
    "\n",
    "    # Check that target path exists, create if not\n",
    "    check_path(model_path)\n",
    "\n",
    "    for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "        current_model_file = get_model_filename(fold=fold, path=model_path)\n",
    "        if not os.path.isfile(current_model_file) or overwrite:\n",
    "            # Load normalizer\n",
    "            feature_normalizer_filename = get_feature_normalizer_filename(fold=fold, path=feature_normalizer_path)\n",
    "            if os.path.isfile(feature_normalizer_filename):\n",
    "                normalizer = load_data(feature_normalizer_filename)\n",
    "            else:\n",
    "                raise IOError(\"Feature normalizer not found [%s]\" % feature_normalizer_filename)\n",
    "\n",
    "            # Initialize model container\n",
    "            model_container = {'normalizer': normalizer, 'models': {}}\n",
    "\n",
    "            # Collect training examples\n",
    "            file_count = len(dataset.train(fold))\n",
    "            data = {}\n",
    "            for item_id, item in enumerate(dataset.train(fold)):\n",
    "                progress(title_text='Collecting data',\n",
    "                         fold=fold,\n",
    "                         percentage=(float(item_id) / file_count),\n",
    "                         note=os.path.split(item['file'])[1])\n",
    "\n",
    "                # Load features\n",
    "                feature_filename = get_feature_filename(audio_file=item['file'], path=feature_path)\n",
    "                if os.path.isfile(feature_filename):\n",
    "                    feature_data = load_data(feature_filename)['feat']\n",
    "                else:\n",
    "                    raise IOError(\"Features not found [%s]\" % (item['file']))\n",
    "\n",
    "                # Scale features\n",
    "                feature_data = model_container['normalizer'].normalize(feature_data)\n",
    "\n",
    "                # Audio error removal\n",
    "                if clean_audio_errors:\n",
    "                    current_errors = dataset.file_error_meta(item['file'])\n",
    "                    if current_errors:\n",
    "                        removal_mask = numpy.ones((feature_data.shape[0]), dtype=bool)\n",
    "                        for error_event in current_errors:\n",
    "                            onset_frame = int(numpy.floor(error_event['event_onset'] / feature_params['hop_length_seconds']))\n",
    "                            offset_frame = int(numpy.ceil(error_event['event_offset'] / feature_params['hop_length_seconds']))\n",
    "                            if offset_frame > feature_data.shape[0]:\n",
    "                                offset_frame = feature_data.shape[0]\n",
    "                            removal_mask[onset_frame:offset_frame] = False\n",
    "                        feature_data = feature_data[removal_mask, :]\n",
    "\n",
    "                # Store features per class label\n",
    "                if item['scene_label'] not in data:\n",
    "                    data[item['scene_label']] = feature_data\n",
    "                else:\n",
    "                    data[item['scene_label']] = numpy.vstack((data[item['scene_label']], feature_data))\n",
    "\n",
    "            # Train models for each class\n",
    "            for label in data:\n",
    "                progress(title_text='Train models',\n",
    "                         fold=fold,\n",
    "                         note=label)\n",
    "                if classifier_method == 'gmm':\n",
    "                    model_container['models'][label] = mixture.GMM(**classifier_params).fit(data[label])\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown classifier method [\"+classifier_method+\"]\")\n",
    "\n",
    "            # Save models\n",
    "            save_data(current_model_file, model_container)\n",
    "\n",
    "\n",
    "def do_system_testing(dataset, result_path, feature_path, model_path, feature_params,\n",
    "                      dataset_evaluation_mode='folds', classifier_method='gmm', clean_audio_errors=False, overwrite=False):\n",
    "    \"\"\"System testing.\n",
    "\n",
    "    If extracted features are not found from disk, they are extracted but not saved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : class\n",
    "        dataset class\n",
    "\n",
    "    result_path : str\n",
    "        path where the results are saved.\n",
    "\n",
    "    feature_path : str\n",
    "        path where the features are saved.\n",
    "\n",
    "    model_path : str\n",
    "        path where the models are saved.\n",
    "\n",
    "    feature_params : dict\n",
    "        parameter dict\n",
    "\n",
    "    dataset_evaluation_mode : str ['folds', 'full']\n",
    "        evaluation mode, 'full' all material available is considered to belong to one fold.\n",
    "        (Default value='folds')\n",
    "\n",
    "    classifier_method : str ['gmm']\n",
    "        classifier method, currently only GMM supported\n",
    "        (Default value='gmm')\n",
    "\n",
    "    clean_audio_errors : bool\n",
    "        Remove audio errors from the training data\n",
    "        (Default value=False)\n",
    "\n",
    "    overwrite : bool\n",
    "        overwrite existing models\n",
    "        (Default value=False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    ValueError\n",
    "        classifier_method is unknown.\n",
    "\n",
    "    IOError\n",
    "        Model file not found.\n",
    "        Audio file not found.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if classifier_method != 'gmm':\n",
    "        raise ValueError(\"Unknown classifier method [\"+classifier_method+\"]\")\n",
    "\n",
    "    # Check that target path exists, create if not\n",
    "    check_path(result_path)\n",
    "\n",
    "    for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "        current_result_file = get_result_filename(fold=fold, path=result_path)\n",
    "        if not os.path.isfile(current_result_file) or overwrite:\n",
    "            results = []\n",
    "\n",
    "            # Load class model container\n",
    "            model_filename = get_model_filename(fold=fold, path=model_path)\n",
    "            if os.path.isfile(model_filename):\n",
    "                model_container = load_data(model_filename)\n",
    "            else:\n",
    "                raise IOError(\"Model file not found [%s]\" % model_filename)\n",
    "\n",
    "            file_count = len(dataset.test(fold))\n",
    "            for file_id, item in enumerate(dataset.test(fold)):\n",
    "                progress(title_text='Testing',\n",
    "                         fold=fold,\n",
    "                         percentage=(float(file_id) / file_count),\n",
    "                         note=os.path.split(item['file'])[1])\n",
    "                \n",
    "                # Load features\n",
    "                feature_filename = get_feature_filename(audio_file=item['file'], path=feature_path)\n",
    "\n",
    "                if os.path.isfile(feature_filename):\n",
    "                    feature_data = load_data(feature_filename)['feat']\n",
    "                else:\n",
    "                    # Load audio\n",
    "                    if os.path.isfile(dataset.relative_to_absolute_path(item['file'])):\n",
    "                        y, fs = load_audio(filename=dataset.relative_to_absolute_path(item['file']), mono=True, fs=feature_params['fs'])\n",
    "                    else:\n",
    "                        raise IOError(\"Audio file not found [%s]\" % (item['file']))\n",
    "\n",
    "                    feature_data = feature_extraction(y=y,\n",
    "                                                      fs=fs,\n",
    "                                                      include_mfcc0=feature_params['include_mfcc0'],\n",
    "                                                      include_delta=feature_params['include_delta'],\n",
    "                                                      include_acceleration=feature_params['include_acceleration'],\n",
    "                                                      mfcc_params=feature_params['mfcc'],\n",
    "                                                      delta_params=feature_params['mfcc_delta'],\n",
    "                                                      acceleration_params=feature_params['mfcc_acceleration'],\n",
    "                                                      statistics=False)['feat']\n",
    "\n",
    "                # Scale features\n",
    "                feature_data = model_container['normalizer'].normalize(feature_data)\n",
    "\n",
    "                if clean_audio_errors:\n",
    "                    current_errors = dataset.file_error_meta(item['file'])\n",
    "                    if current_errors:\n",
    "                        removal_mask = numpy.ones((feature_data.shape[0]), dtype=bool)\n",
    "                        for error_event in current_errors:\n",
    "                            onset_frame = int(numpy.floor(error_event['event_onset'] / feature_params['hop_length_seconds']))\n",
    "                            offset_frame = int(numpy.ceil(error_event['event_offset'] / feature_params['hop_length_seconds']))\n",
    "                            if offset_frame > feature_data.shape[0]:\n",
    "                                offset_frame = feature_data.shape[0]\n",
    "                            removal_mask[onset_frame:offset_frame] = False\n",
    "                        feature_data = feature_data[removal_mask, :]\n",
    "\n",
    "                # Do classification for the block\n",
    "                if classifier_method == 'gmm':\n",
    "                    current_result = do_classification_gmm(feature_data, model_container)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown classifier method [\"+classifier_method+\"]\")\n",
    "\n",
    "                # Store the result\n",
    "                results.append((dataset.absolute_to_relative(item['file']), current_result))\n",
    "\n",
    "            # Save testing results\n",
    "            with open(current_result_file, 'wt') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                for result_item in results:\n",
    "                    writer.writerow(result_item)\n",
    "\n",
    "\n",
    "def do_classification_gmm(feature_data, model_container):\n",
    "    \"\"\"GMM classification for give feature matrix\n",
    "\n",
    "    model container format:\n",
    "\n",
    "    {\n",
    "        'normalizer': normalizer class\n",
    "        'models' :\n",
    "            {\n",
    "                'office' : mixture.GMM class\n",
    "                'home' : mixture.GMM class\n",
    "                ...\n",
    "            }\n",
    "    }\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_data : numpy.ndarray [shape=(t, feature vector length)]\n",
    "        feature matrix\n",
    "\n",
    "    model_container : dict\n",
    "        model container\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : str\n",
    "        classification result as scene label\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize log-likelihood matrix to -inf\n",
    "    logls = numpy.empty(len(model_container['models']))\n",
    "    logls.fill(-numpy.inf)\n",
    "\n",
    "    for label_id, label in enumerate(model_container['models']):\n",
    "        logls[label_id] = numpy.sum(model_container['models'][label].score(feature_data))\n",
    "\n",
    "    classification_result_id = numpy.argmax(logls)\n",
    "    return model_container['models'].keys()[classification_result_id]\n",
    "\n",
    "\n",
    "def do_system_evaluation(dataset, result_path, dataset_evaluation_mode='folds'):\n",
    "    \"\"\"System evaluation. Testing outputs are collected and evaluated. Evaluation results are printed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : class\n",
    "        dataset class\n",
    "\n",
    "    result_path : str\n",
    "        path where the results are saved.\n",
    "\n",
    "    dataset_evaluation_mode : str ['folds', 'full']\n",
    "        evaluation mode, 'full' all material available is considered to belong to one fold.\n",
    "        (Default value='folds')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nothing\n",
    "\n",
    "    Raises\n",
    "    -------\n",
    "    IOError\n",
    "        Result file not found\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dcase2016_scene_metric = DCASE2016_SceneClassification_Metrics(class_list=dataset.scene_labels)\n",
    "    results_fold = []\n",
    "    for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "        dcase2016_scene_metric_fold = DCASE2016_SceneClassification_Metrics(class_list=dataset.scene_labels)\n",
    "        results = []\n",
    "        result_filename = get_result_filename(fold=fold, path=result_path)\n",
    "\n",
    "        if os.path.isfile(result_filename):\n",
    "            with open(result_filename, 'rt') as f:\n",
    "                for row in csv.reader(f, delimiter='\\t'):\n",
    "                    results.append(row)\n",
    "        else:\n",
    "            raise IOError(\"Result file not found [%s]\" % result_filename)\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for result in results:\n",
    "            y_true.append(dataset.file_meta(result[0])[0]['scene_label'])\n",
    "            y_pred.append(result[1])\n",
    "        dcase2016_scene_metric.evaluate(system_output=y_pred, annotated_ground_truth=y_true)\n",
    "        dcase2016_scene_metric_fold.evaluate(system_output=y_pred, annotated_ground_truth=y_true)\n",
    "        results_fold.append(dcase2016_scene_metric_fold.results())\n",
    "    results = dcase2016_scene_metric.results()\n",
    "\n",
    "    print (\"  File-wise evaluation, over %d folds\" % dataset.fold_count)\n",
    "    fold_labels = ''\n",
    "    separator = '     =====================+======+======+==========+  +'\n",
    "    if dataset.fold_count > 1:\n",
    "        for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "            fold_labels += \" {:8s} |\".format('Fold'+str(fold))\n",
    "            separator += \"==========+\"\n",
    "    print (\"     {:20s} | {:4s} : {:4s} | {:8s} |  |\".format('Scene label', 'Nref', 'Nsys', 'Accuracy')+fold_labels)\n",
    "    print (separator)\n",
    "    for label_id, label in enumerate(sorted(results['class_wise_accuracy'])):\n",
    "        fold_values = ''\n",
    "        if dataset.fold_count > 1:\n",
    "            for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "                fold_values += \" {:5.1f} %  |\".format(results_fold[fold-1]['class_wise_accuracy'][label] * 100)\n",
    "        print (\"     {:20s} | {:4d} : {:4d} | {:5.1f} %  |  |\".format(label,\n",
    "                                                                     results['class_wise_data'][label]['Nref'],\n",
    "                                                                     results['class_wise_data'][label]['Nsys'],\n",
    "                                                                     results['class_wise_accuracy'][label] * 100)+fold_values)\n",
    "    print (separator)\n",
    "    fold_values = ''\n",
    "    if dataset.fold_count > 1:\n",
    "        for fold in dataset.folds(mode=dataset_evaluation_mode):\n",
    "            fold_values += \" {:5.1f} %  |\".format(results_fold[fold-1]['overall_accuracy'] * 100)\n",
    "\n",
    "    print (\"     {:20s} | {:4d} : {:4d} | {:5.1f} %  |  |\".format('Overall accuracy',\n",
    "                                                                 results['Nref'],\n",
    "                                                                 results['Nsys'],\n",
    "                                                                 results['overall_accuracy'] * 100)+fold_values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        sys.exit(main(sys.argv))\n",
    "    except (ValueError, IOError) as e:\n",
    "        sys.exit(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
